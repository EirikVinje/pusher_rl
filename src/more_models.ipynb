{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO, A2C, HER, SAC, TD3\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "path = os.getcwd()\n",
    "savepath = os.path.join(path, os.pardir)\n",
    "num_episodes = 200_000\n",
    "episode_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO\n",
    "env = gym.make(\"Pusher-v4\", render_mode='rgb_array', max_episode_steps=episode_steps)\n",
    "vec_env = make_vec_env(lambda:env, n_envs=1)\n",
    "\n",
    "params={'n_steps': 483, 'gamma': 0.9733319984142623, 'learning_rate': 6.249837257325398e-05, 'ent_coef': 6.2177665117053e-05, 'clip_range': 0.20674514787972498, 'n_epochs': 18, 'gae_lambda': 0.9180827852846369, 'max_grad_norm': 0.7932345709898225, 'vf_coef': 0.5888144289366819}\n",
    "\n",
    "model_ppo = PPO(\"MlpPolicy\", vec_env, \n",
    "            verbose=1,\n",
    "            batch_size=32,\n",
    "            n_steps=params['n_steps'],\n",
    "            gamma=params['gamma'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            ent_coef=params['ent_coef'],\n",
    "            clip_range=params['clip_range'],\n",
    "            n_epochs=params['n_epochs'],\n",
    "            gae_lambda=params['gae_lambda'],\n",
    "            max_grad_norm=params['max_grad_norm'],\n",
    "            vf_coef=params['vf_coef'],\n",
    "            tensorboard_log=\"./ppo_tensorboard_log/\")\n",
    "\n",
    "model_ppo.learn(total_timesteps=200_000,log_interval=10,progress_bar=True)\n",
    "model_ppo.save(f\"{savepath}/models/ppo_{num_episodes/1000}K_{episode_steps}ep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to test:\n",
    "\n",
    "- A2C\n",
    "- HER\n",
    "- SAC\n",
    "- TD3\n",
    "\n",
    "If time:\n",
    "- TRPO\n",
    "- RecurrentPPO\n",
    "- ARS\n",
    "- TQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C\n",
    "env = gym.make(\"Pusher-v4\", render_mode='rgb_array', max_episode_steps=episode_steps)\n",
    "vec_env = make_vec_env(lambda:env, n_envs=1)\n",
    "\n",
    "model_a2c = A2C(\"MlpPolicy\", vec_env, tensorboard_log=\"./ppo_tensorboard_log/\")\n",
    "model_a2c.learn(num_episodes, log_interval=10, progress_bar=True)\n",
    "model_a2c.save(f\"{savepath}/models/a2c_{num_episodes/1000}K_{episode_steps}ep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HER\n",
    "env = gym.make(\"Pusher-v4\", render_mode='rgb_array', max_episode_steps=episode_steps)\n",
    "vec_env = make_vec_env(lambda:env, n_envs=1)\n",
    "\n",
    "model_her = HER(\"MlpPolicy\", vec_env, \n",
    "                model_class='tqc', \n",
    "                n_sampled_goal=4, \n",
    "                goal_selection_strategy=\"future\", \n",
    "                buffer_size=1000000, \n",
    "                batch_size=2048, \n",
    "                gamma=0.95, \n",
    "                learning_rate=1e-3, \n",
    "                tau=0.05, \n",
    "                policy_kwargs=dict(n_critics=2, net_arch=[512, 512, 512]), \n",
    "                online_sampling=True, \n",
    "                verbose=1, \n",
    "                tensorboard_log=\"./ppo_tensorboard_log/\")\n",
    "\n",
    "model_her.learn(num_episodes, log_interval=10, progress_bar=True)\n",
    "model_her.save(f\"{savepath}/models/her_{num_episodes/1000}K_{episode_steps}ep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC\n",
    "env = gym.make(\"Pusher-v4\", render_mode='rgb_array', max_episode_steps=episode_steps)\n",
    "vec_env = make_vec_env(lambda:env, n_envs=1)\n",
    "\n",
    "model_sac = SAC(\"MlpPolicy\", vec_env, tensorboard_log=\"./ppo_tensorboard_log/\")\n",
    "model_sac.learn(num_episodes, log_interval=10, progress_bar=True)\n",
    "model_sac.save(f\"{savepath}/models/sac_{num_episodes/1000}K_{episode_steps}ep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD3\n",
    "env = gym.make(\"Pusher-v4\", render_mode='rgb_array', max_episode_steps=episode_steps)\n",
    "vec_env = make_vec_env(lambda:env, n_envs=1)\n",
    "\n",
    "model_td3 = TD3(\"MlpPolicy\", vec_env, tensorboard_log=\"./ppo_tensorboard_log/\")\n",
    "model_td3.learn(num_episodes, log_interval=10, progress_bar=True)\n",
    "model_td3.save(f\"{savepath}/models/td3_{num_episodes/1000}K_{episode_steps}ep\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable-baselines3 contrib library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TRPO, RecurrentPPO, ARS, TQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRPO\n",
    "env = gym.make(\"Pusher-v4\", render_mode='rgb_array', max_episode_steps=episode_steps)\n",
    "vec_env = make_vec_env(lambda:env, n_envs=1)\n",
    "\n",
    "model_trpo = TRPO(\"MlpPolicy\", vec_env, tensorboard_log=\"./ppo_tensorboard_log/\")\n",
    "model_trpo.learn(num_episodes, log_interval=10, progress_bar=True)\n",
    "model_trpo.save(f\"{savepath}/models/trpo_{num_episodes/1000}K_{episode_steps}ep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecurrentPPO\n",
    "env = gym.make(\"Pusher-v4\", render_mode='rgb_array', max_episode_steps=episode_steps)\n",
    "vec_env = make_vec_env(lambda:env, n_envs=1)\n",
    "\n",
    "model_rppo = RecurrentPPO(\"MlpLstmPolicy\", vec_env, \n",
    "            verbose=1,\n",
    "            batch_size=32,\n",
    "            n_steps=params['n_steps'],\n",
    "            gamma=params['gamma'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            ent_coef=params['ent_coef'],\n",
    "            clip_range=params['clip_range'],\n",
    "            n_epochs=params['n_epochs'],\n",
    "            gae_lambda=params['gae_lambda'],\n",
    "            max_grad_norm=params['max_grad_norm'],\n",
    "            vf_coef=params['vf_coef'],\n",
    "            tensorboard_log=\"./ppo_tensorboard_log/\")\n",
    "model_rppo.learn(num_episodes, log_interval=10, progress_bar=True)\n",
    "model_rppo.save(f\"{savepath}/models/rppo_{num_episodes/1000}K_{episode_steps}ep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARS\n",
    "env = gym.make(\"Pusher-v4\", render_mode='rgb_array', max_episode_steps=episode_steps)\n",
    "vec_env = make_vec_env(lambda:env, n_envs=1)\n",
    "\n",
    "model_ars = ARS(\"MlpPolicy\", vec_env, tensorboard_log=\"./ppo_tensorboard_log/\")\n",
    "model_ars.learn(num_episodes, log_interval=10, progress_bar=True)\n",
    "model_ars.save(f\"{savepath}/models/ars_{num_episodes/1000}K_{episode_steps}ep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TQC\n",
    "env = gym.make(\"Pusher-v4\", render_mode='rgb_array', max_episode_steps=episode_steps)\n",
    "vec_env = make_vec_env(lambda:env, n_envs=1)\n",
    "\n",
    "model_tqc = TQC(\"MlpPolicy\", vec_env, tensorboard_log=\"./ppo_tensorboard_log/\")\n",
    "model_tqc.learn(num_episodes, log_interval=10, progress_bar=True)\n",
    "model_tqc.save(f\"{savepath}/models/tqc_{num_episodes/1000}K_{episode_steps}ep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [3559, 3216, 7890, 5242, 4924, 3588, 722, 8119]\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    vec_env.seed(seed)\n",
    "    state = vec_env.reset()\n",
    "    while True:\n",
    "        action, _states = model.predict(state)\n",
    "        state, rewards, dones, info = vec_env.step(action)\n",
    "        vec_env.render(\"human\")\n",
    "\n",
    "        if dones:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pusher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
